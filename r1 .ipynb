{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true,"gpuType":"T4","authorship_tag":"ABX9TyPynw7LTFa2iyzJbWXrZDLz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"v2YNY63aqrqd"},"outputs":[],"source":["!pip install unsloth vllm trl"]},{"cell_type":"code","source":["# Step 1: Patch GRPO for reinforcement learning\n","from unsloth import FastLanguageModel, PatchFastRL, is_bfloat16_supported\n","from trl import GRPOConfig, GRPOTrainer\n","PatchFastRL(\"GRPO\", FastLanguageModel)\n","\n","import logging\n","# Configure logging\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)"],"metadata":{"id":"HqHlHxoQtGa3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","# Step 2: Define model and training parameters\n","MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n","MAX_SEQ_LENGTH = 1024\n","LORA_RANK = 64 # Efficient tuning\n","GPU_MEMORY_UTIL = 0.6  # Optimize GPU usage"],"metadata":{"id":"BVRx3bzhxqNP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 3: Load model with LoRA fine-tuning\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name=MODEL_NAME,\n","    max_seq_length=MAX_SEQ_LENGTH,\n","    load_in_4bit=True,\n","    fast_inference=True,\n","    max_lora_rank=LORA_RANK,\n","    gpu_memory_utilization=GPU_MEMORY_UTIL,\n",")\n","\n","# Apply LoRA optimization\n","model = FastLanguageModel.get_peft_model(\n","    model,\n","    r=LORA_RANK,\n","    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","        \"gate_proj\", \"up_proj\", \"down_proj\",],  # Optimized layer selection\n","    lora_alpha=LORA_RANK,\n","    use_gradient_checkpointing=\"unsloth\",\n","    random_state=42,\n","\n",")"],"metadata":{"id":"L-CE4vCUAAUw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","from datasets import load_dataset, Dataset\n","\n","# Load and prep dataset\n","SYSTEM_PROMPT = \"\"\"\n","Respond in the following format:\n","<reasoning>\n","...\n","</reasoning>\n","<answer>\n","...\n","</answer>\n","\"\"\"\n","\n","XML_COT_FORMAT = \"\"\"\\\n","<reasoning>\n","{reasoning}\n","</reasoning>\n","<answer>\n","{answer}\n","</answer>\n","\"\"\"\n","\n","def extract_xml_answer(text: str) -> str:\n","    answer = text.split(\"<answer>\")[-1]\n","    answer = answer.split(\"</answer>\")[0]\n","    return answer.strip()\n","\n","def extract_hash_answer(text: str) -> str | None:\n","    if \"####\" not in text:\n","        return None\n","    return text.split(\"####\")[1].strip()\n","\n","# uncomment middle messages for 1-shot prompting\n","def get_gsm8k_questions(split = \"train\") -> Dataset:\n","    data = load_dataset('openai/gsm8k', 'main')[split] # type: ignore\n","    data = data.map(lambda x: { # type: ignore\n","        'prompt': [\n","            {'role': 'system', 'content': SYSTEM_PROMPT},\n","            {'role': 'user', 'content': x['question']}\n","        ],\n","        'answer': extract_hash_answer(x['answer'])\n","    }) # type: ignore\n","    return data # type: ignore\n","\n","dataset = get_gsm8k_questions()"],"metadata":{"id":"PS1g5wG29P7r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sentence_transformers import SentenceTransformer, util\n","# Step 5: Define optimized reward functions\n","embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n","#\n","def reward_correctness(completions, answer, **kwargs):\n","    \"\"\"Reward based on semantic similarity instead of exact matching.\"\"\"\n","    completion_texts = [completion[0]['content'].strip() for completion in completions]\n","    answer_texts = [a.strip() for a in answer]\n","\n","    embeddings_comp = embed_model.encode(completion_texts, convert_to_tensor=True)\n","    embeddings_ans = embed_model.encode(answer_texts, convert_to_tensor=True)\n","    similarities = util.pytorch_cos_sim(embeddings_comp, embeddings_ans).diagonal()\n","\n","    return [float(sim) for sim in similarities]\n","\n","#\n","def soft_format_reward_func(completions, **kwargs) -> list[float]:\n","    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n","    pattern = r\".*?\\s*.*?\"\n","    responses = [completion[0][\"content\"] for completion in completions]\n","    matches = [re.match(pattern, r) for r in responses]\n","    return [0.5 if match else 0.0 for match in matches]\n","#\n","def strict_format_reward_func(completions, **kwargs):\n","    \"\"\"Ensure response follows strict XML-based reasoning/answer format.\"\"\"\n","    pattern = r\"^<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>$\"\n","    responses = [completion[0][\"content\"] for completion in completions]\n","    return [0.5 if re.match(pattern, r, re.DOTALL) else 0.0 for r in responses]\n","#\n","def int_reward_func(completions, **kwargs):\n","    \"\"\"Ensure numerical answers when required.\"\"\"\n","    responses = [completion[0]['content'] for completion in completions]\n","    return [0.5 if any(char.isdigit() for char in r) else 0.0 for r in responses]\n","#\n","def count_xml(text) -> float:\n","    count = 0.0\n","    if text.count(\"\\n\") == 1:\n","        count += 0.125\n","    if text.count(\"\\n\\n\") == 1:\n","        count += 0.125\n","    if text.count(\"\\n\\n\") == 1:\n","        count += 0.125\n","        count -= len(text.split(\"\\n\\n\")[-1])*0.001\n","    if text.count(\"\\n\") == 1:\n","        count += 0.125\n","        count -= (len(text.split(\"\\n\")[-1]) - 1)*0.001\n","    return count\n","#\n","def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n","    contents = [completion[0][\"content\"] for completion in completions]\n","    return [count_xml(c) for c in contents]\n","\n","# extra\n","def reward_stepwise_accuracy(completions, **kwargs):\n","    \"\"\"Reward answers that follow correct reasoning steps.\"\"\"\n","    return [0.7 if \"<reasoning>\" in completion[0]['content'] else 0.0 for completion in completions]\n","\n","def reward_length_penalty(completions, **kwargs):\n","    \"\"\"Penalize responses that are too long or too short.\"\"\"\n","    return [-0.2 if len(completion[0]['content'].split()) > 150 else 0.2 for completion in completions]"],"metadata":{"id":"mcPjJAM74eYs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 6: Configure training parameters\n","from trl import GRPOConfig, GRPOTrainer\n","training_args = GRPOConfig(\n","    use_vllm = True, # use vLLM for fast inference!\n","    learning_rate = 5e-6,\n","    adam_beta1 = 0.9,\n","    adam_beta2 = 0.99,\n","    weight_decay = 0.1,\n","    warmup_ratio = 0.1,\n","    lr_scheduler_type = \"cosine\",\n","    optim = \"adamw_8bit\",\n","    logging_steps = 1,\n","    bf16 = is_bfloat16_supported(),\n","    fp16 = not is_bfloat16_supported(),\n","    per_device_train_batch_size = 1,\n","    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n","    num_generations = 8, # Decrease if out of memory\n","    max_prompt_length = 256,\n","    max_completion_length = 200,\n","    # num_train_epochs = 1, # Set to 1 for a full training run\n","    max_steps = 150,\n","    save_steps = 150,\n","    max_grad_norm = 0.1,\n","    report_to = \"none\", # Can use Weights & Biases\n","    output_dir = \"outputs\",\n",")"],"metadata":{"id":"NLXIoKxxBtk3"},"execution_count":null,"outputs":[]},{"source":["# Step 7: Initialize and train model\n","logger.info(\"Initializing trainer...\")\n","\n","trainer = GRPOTrainer(\n","    model = model,\n","    processing_class = tokenizer,\n","    reward_funcs = [\n","        strict_format_reward_func,\n","        int_reward_func,\n","        reward_correctness, # Assuming this is what you intended\n","        reward_stepwise_accuracy, # Adding this reward function for more robust training\n","        reward_length_penalty, # Adding this to penalize overly long responses\n","        xmlcount_reward_func, # Adding this to penalize overly long responses\n","        soft_format_reward_func, # Adding this to penalize overly long response\n","    ],\n","    args = training_args,\n","    train_dataset = dataset,\n",")\n","trainer.train()\n","\n","\n","\n"],"cell_type":"code","metadata":{"id":"-Nb-PoEoHu0i"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YC9BBT0RESln"},"outputs":[],"source":["model.save_lora(\"grpo_saved_lora\")"]},{"cell_type":"code","source":["# model.push_to_hub_merged(\"rushigulum/GRPO4\", tokenizer, save_method = \"merged_16bit\")"],"metadata":{"id":"zHmT2nb7_5pr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install huggingface_hub"],"metadata":{"id":"VsUCSLXmq_no"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","notebook_login()\n"],"metadata":{"id":"9AhFqM6YrlKO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","os.listdir(\"/content\")\n"],"metadata":{"id":"oycb1lXkJrP2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from huggingface_hub import HfApi\n","\n","api = HfApi()\n","api.create_repo(\"rushigulum/grpo\", private=False)\n","\n","api.upload_folder(\n","    folder_path=\"/content/grpo_saved_lora\",  # Change this to your model's path\n","    repo_id=\"rushigulum/grpo\",\n",")\n"],"metadata":{"id":"eQmhSA0_Hev5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from huggingface_hub import HfApi\n","\n","repo_name = \"rushigulum\"\n","# Ensure model_path points to a directory containing the model files\n","model_path = \"grpo_saved_lora\"  # Changed from 'optimized_outputs/best_model'\n","\n","# Upload model\n","api = HfApi()\n","api.create_repo(repo_name, exist_ok=True)\n","api.upload_folder(folder_path=model_path, repo_id=repo_name, repo_type=\"model\")"],"metadata":{"id":"s8G7DFiir4ze"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.push_to_hub_merged(\"rushigulum/GRPO4\", tokenizer, save_method = \"lora\")"],"metadata":{"id":"LKvrL4dNviLO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers torch"],"metadata":{"id":"vFVigtfGzkj5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","\n","# Replace with your Hugging Face repo ID\n","MODEL_REPO = \"rushigulum/q3b-grpo\"\n","\n","# Load the model and tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_REPO)\n","model = AutoModelForCausalLM.from_pretrained(MODEL_REPO, torch_dtype=torch.float16, device_map=\"auto\")\n"],"metadata":{"id":"cEVVwu4mzoqp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_response(prompt):\n","    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n","    output_ids = model.generate(input_ids, max_length=200, temperature=0.7)\n","    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","\n","# Example Test\n","prompt = \"What is the capital of France?\"\n","response = generate_response(prompt)\n","print(\"Model Response:\", response)\n"],"metadata":{"id":"kP2DMMrDzpv5"},"execution_count":null,"outputs":[]}]}